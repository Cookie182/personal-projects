{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "executive-thanksgiving",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T14:14:00.800845Z",
     "start_time": "2021-02-16T14:14:00.791843Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "from matplotlib.gridspec import GridSpec as gs\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('dark')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score, plot_confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-practice",
   "metadata": {},
   "source": [
    "#### Gathering basic info about subject structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spread-andrews",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T10:22:25.752420Z",
     "start_time": "2021-02-16T10:22:01.539429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the types of tests (seperated by a space): Lab Assignment Mid-Term Project Final\n",
      "How many tests for Lab?: 8\n",
      "How many tests for Assignment?: 4\n",
      "How many tests for Mid-Term?: 1\n",
      "How many tests for Project?: 1\n",
      "How many tests for Final?: 1\n",
      "Lab out of how many marks?: 10\n",
      "Assignment out of how many marks?: 20\n",
      "Mid-Term out of how many marks?: 50\n",
      "Project out of how many marks?: 100\n",
      "Final out of how many marks?: 100\n",
      "What is the weightage for Lab?: 20\n",
      "What is the weightage for Assignment?: 10\n",
      "What is the weightage for Mid-Term?: 10\n",
      "What is the weightage for Project?: 40\n",
      "What is the weightage for Final?: 20\n",
      "What is the passing percentage threshold?: 60\n",
      "Which of these tests ('Lab', 'Assignment', 'Mid-Term', 'Project', 'Final') is the final test?: Final\n"
     ]
    }
   ],
   "source": [
    "test_type = tuple(input(\"Enter the types of tests (seperated by a space): \").split()) # type of evaluations\n",
    "test_amount = tuple(int(input(f\"How many tests for {x}?: \")) for x in test_type) # amount of tests per evaluation\n",
    "max_mark = tuple(int(input(f\"{x} out of how many marks?: \")) for x in test_type) # maximum marks for each type of tests\n",
    "\n",
    "while True:\n",
    "    weightage = tuple(float(input(f\"What is the weightage for {x}?: \"))/100 for x in test_type)\n",
    "    if np.sum(weightage) == 1.0:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Make sure the weightage for all tests add up to 1.0!\\n\")\n",
    "pass_percent = float(input(\"What is the passing percentage threshold?: \"))/100\n",
    "\n",
    "while True:\n",
    "    final_test_name = input(f\"Which of these tests {test_type} is the final test?: \")\n",
    "    if final_test_name in test_type:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Make sure the name of the final test is enterred correctly!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legitimate-birthday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T10:22:25.782430Z",
     "start_time": "2021-02-16T10:22:25.754436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>how many</th>\n",
       "      <th>max marks</th>\n",
       "      <th>weightage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Lab</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Assignment</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mid-Term</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Project</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            how many  max marks  weightage\n",
       "test type                                 \n",
       "Lab                8         10        0.2\n",
       "Assignment         4         20        0.1\n",
       "Mid-Term           1         50        0.1\n",
       "Project            1        100        0.4\n",
       "Final              1        100        0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = pd.DataFrame(index=test_type)\n",
    "tests['how many'] = test_amount\n",
    "tests['max marks'] = max_mark\n",
    "tests['weightage'] = weightage\n",
    "tests.index.name = 'test type'\n",
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "demanding-expansion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T13:19:27.531450Z",
     "start_time": "2021-02-16T13:19:27.511452Z"
    }
   },
   "outputs": [],
   "source": [
    "def grades(test_type, test_amount, max_mark, weightage, pass_percent, final_test_name, n=1000, graphs=False): # grades generator\n",
    "    \"\"\"Func that generates train/test data for the classifier/regressor and returns the trained classifer/regressor\"\"\"\n",
    "    df = pd.DataFrame(index=range(1, n+1)) # making the dataframe and generating dummy marks\n",
    "    df.index.name = 'Student'\n",
    "    for x in range(len(test_type)):\n",
    "        m = max_mark[x] # storing max marks for each type of test \n",
    "        if test_amount[x] > 1:\n",
    "            for y in range(1, test_amount[x] + 1):\n",
    "                df[f\"{test_type[x]} {y}\"] = [round(x) for x in (ss.truncnorm.rvs(((0 - int(m * 0.65)) / (m//3)),\n",
    "                                         ((m - int(m * 0.65)) / (m//3)), \n",
    "                                         loc=round(m * 0.65, 0), scale=(m//3), size=n))]\n",
    "        else:\n",
    "            for y in range(1, test_amount[x] + 1):\n",
    "                df[f\"{test_type[x]}\"] = [round(x) for x in (ss.truncnorm.rvs(((0 - int(m * 0.65)) / (m//3)),\n",
    "                                         ((m - int(m * 0.65)) / (m//3)), \n",
    "                                         loc=round(m * 0.65, 0), scale=(m//3), size=n))]  \n",
    "\n",
    "    # calculating total grade weight weightage\n",
    "    df['Total %'] = [0] * len(df)\n",
    "    for x in range(len(test_type)):\n",
    "        df['Total %'] += round((df.filter(regex=test_type[x]).sum(axis=1) / (test_amount[x] * max_mark[x])) * weightage[x], 2)\n",
    "\n",
    "    # determining pass/fail\n",
    "    df['Pass/Fail'] = [\"Pass\" if x >= pass_percent else \"Fail\" for x in df['Total %']]\n",
    "    \n",
    "    print(f\"\\nStudents passed -> {len(df[df['Pass/Fail'] == 'Pass'])}\\\n",
    "    \\nStudents Failed -> {len(df[df['Pass/Fail'] == 'Fail'])}\\n\")\n",
    "    \n",
    "    X = df.drop(['Pass/Fail', 'Total %', 'Final'], axis=1)\n",
    "    y = df[['Pass/Fail', 'Total %']]\n",
    "    y['Pass/Fail'] = LabelEncoder().fit_transform(y['Pass/Fail'])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y['Pass/Fail'])\n",
    "    \n",
    "    passfail = make_pipeline(StandardScaler(), LogisticRegressionCV(Cs=np.arange(0.1, 0.6, 0.1),\n",
    "                                                                    cv=RepeatedStratifiedKFold(n_splits=10, random_state=7),\n",
    "                                                                   max_iter=1000, n_jobs=-1, refit=True, \n",
    "                                                                    random_state=7,\n",
    "                                                                   class_weight='balanced')).fit(X_test, \n",
    "                                                                                                 y_test['Pass/Fail'])\n",
    "    \n",
    "    if graphs == True:\n",
    "        for x in range(len(test_type)):\n",
    "            if test_amount[x] > 1: # plotting grade distribution for tests with more than one evaluation of that type\n",
    "                if test_amount[x] % 2 != 0:\n",
    "                    y = test_amount[x]+1\n",
    "                else:\n",
    "                    y= test_amount[x]\n",
    "\n",
    "                fig = plt.figure(figsize=(10, 7), constrained_layout=True)\n",
    "                grid = gs(nrows=int(y/2), ncols=2, figure=fig)\n",
    "                for y in range(test_amount[x]):\n",
    "                    ax = fig.add_subplot(grid[y])\n",
    "                    sns.distplot(df.filter(regex=test_type[x]).iloc[:, y], fit=ss.norm, ax=ax, norm_hist=True, color='red',\n",
    "                                hist_kws=dict(edgecolor='black', align='right', color='red'), \n",
    "                                 bins=max_mark[x]//2)\n",
    "                    plt.xticks(size=10)\n",
    "                    plt.yticks(size=12)\n",
    "                    plt.xlabel('Marks', fontdict={'size':13})\n",
    "                    plt.ylabel('Density', fontdict={'size':13})\n",
    "                    plt.title(f\"{test_type[x]} {y+1}\", fontsize=14)\n",
    "                    plt.tight_layout()\n",
    "                fig.suptitle(f\"Grade for {test_type[x]}\", fontsize=15)\n",
    "                grid.tight_layout(fig)\n",
    "                if graphs == True:\n",
    "                    plt.show()\n",
    "            else: # plotting grade distribution for singular evaluation test \n",
    "                plt.figure(figsize=(8, 5))\n",
    "                sns.distplot(df[test_type[x]], fit=ss.norm, norm_hist=True, color='red',\n",
    "                                hist_kws=dict(edgecolor='black', align='right', color='red'), \n",
    "                             bins=max_mark[x]//2)\n",
    "                plt.title(f\"Grade for {test_type[x]}\", fontsize=14)\n",
    "                plt.xlabel('Marks', fontdict={'size':13})\n",
    "                plt.xticks(size=12)\n",
    "                plt.yticks(size=12)\n",
    "                plt.ylabel('Density', fontdict={'size':13})\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "        fig, ax = plt.subplots()\n",
    "        plot_confusion_matrix(passfail, X_test, y_test['Pass/Fail'], labels=[0, 1], \n",
    "                              display_labels=['Fail', 'Pass'], cmap='afmhot', ax=ax)\n",
    "        plt.rcParams.update({'font.size':18})\n",
    "        ax.set_title('Confusion Matrix')\n",
    "        plt.show()\n",
    "    \n",
    "    overallgrade = make_pipeline(StandardScaler(), LinearRegression(n_jobs=-1)).fit(X_test, y_test['Total %'])\n",
    "    \n",
    "    print(\"LogisticRegressionCV classifer:-\")\n",
    "    print(f\"Accuracy -> {round(passfail.score(X_test, y_test['Pass/Fail'])*100, 2)}%\")\n",
    "    print(f\"f1_score -> {round(f1_score(y_test['Pass/Fail'], passfail.predict(X_test))*100, 2)}%\\n\")\n",
    "    \n",
    "    print(\"LinearRegression regressor:-\")\n",
    "    print(f\"Accuracy -> {round(overallgrade.score(X_test, y_test['Total %'])*100, 2)}%\")\n",
    "    \n",
    "    return passfail, overallgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "taken-lounge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T13:19:36.961252Z",
     "start_time": "2021-02-16T13:19:36.715697Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Students passed -> 491    \n",
      "Students Failed -> 509\n",
      "\n",
      "LogisticRegressionCV classifer:-\n",
      "Accuracy -> 94.0%\n",
      "f1_score -> 93.88%\n",
      "\n",
      "LinearRegression regressor:-\n",
      "Accuracy -> 86.47%\n"
     ]
    }
   ],
   "source": [
    "passfail, overallgrade = grades(test_type, test_amount, max_mark, weightage, pass_percent, final_test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "elegant-ballet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-16T14:15:27.500884Z",
     "start_time": "2021-02-16T14:15:27.483647Z"
    }
   },
   "outputs": [],
   "source": [
    "# saving models\n",
    "with open('passfail', 'wb') as f:\n",
    "    pickle.dump(passfail, f)\n",
    "    \n",
    "with open('overallgrade', 'wb') as f:\n",
    "    pickle.dump(overallgrade, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-fault",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
